{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899ce304-a574-4a94-8c58-cad4665311cf",
   "metadata": {},
   "source": [
    "# Anonymizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e6e0ce6-15d3-4301-aec2-bdb1b365e5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cider.datastore import DataStore\n",
    "from cider.anonymizer import Anonymizer\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1ddc5-77ad-4a0a-929d-780371ab675f",
   "metadata": {},
   "source": [
    "Set up the configuration file and load some simulated data, including featurization results if present, using the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c93a535-85df-4949-a93c-09a794873ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/17 11:56:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/17 11:56:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CDR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading recharges...\n",
      "SUCCESS!\n",
      "Loading mobile data...\n",
      "Loading mobile money...\n"
     ]
    }
   ],
   "source": [
    "# This path should point to your cider installation, where configs and data for this demo are located.\n",
    "from pathlib import Path\n",
    "cider_installation_directory = Path('../../cider')\n",
    "\n",
    "datastore = DataStore(config_file_path_string= cider_installation_directory / 'configs' / 'colab_walkthrough' / 'config_anonymize.yml')\n",
    "anonymizer = Anonymizer(datastore=datastore)\n",
    "\n",
    "outputs_path = anonymizer.outputs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc63b4-2cfe-4e7d-8052-1cacb3f4c7e4",
   "metadata": {},
   "source": [
    "## Anonymize input data\n",
    "\n",
    "We can anonymize the five main categories of input data which contain phone numbers: \n",
    "\n",
    "- CDR (calls and texts)\n",
    "- Mobile money transactions\n",
    "- Mobile data transactions\n",
    "- Recharges\n",
    "- Labels (e.g. from a ground-truth survey)\n",
    "\n",
    "It's important to anonymize these using the same anonymization salt, to ensure that all of\n",
    "a given subscriber's data is indexed by the same obfuscated string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55adf165-5f57-45f2-8db3-d2803e154d93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anonymizer.anonymize_cdr()\n",
    "\n",
    "anonymized_cdr = pd.read_csv(outputs_path / 'outputs' / 'cdr.csv')\n",
    "# anonymized_cdr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b502535b-4053-457e-b333-4530ff3c552c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anonymizer.anonymize_mobilemoney()\n",
    "\n",
    "anonymized_mobilemoney = pd.read_csv(outputs_path / 'outputs' / 'mobilemoney.csv')\n",
    "# anonymized_mobilemoney.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5ea037-120a-44af-b879-198078a3822e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anonymizer.anonymize_mobiledata()\n",
    "\n",
    "anonymized_mobiledata = pd.read_csv(outputs_path / 'outputs' / 'mobiledata.csv')\n",
    "# anonymized_mobiledata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50df1ccd-f66c-4d8c-82a2-f9bcc3725c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anonymizer.anonymize_recharges()\n",
    "\n",
    "anonymized_recharges = pd.read_csv(outputs_path / 'outputs' / 'recharges.csv')\n",
    "# anonymized_recharges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56941c6-1878-4fdb-b397-166937504fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anonymizer.anonymize_labels()\n",
    "\n",
    "anonymized_labels = pd.read_csv(outputs_path / 'outputs' / 'labels.csv')\n",
    "# anonymized_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "78c545fc-95bf-4e37-82ed-d7fe1753c166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_cdr_file = pd.read_csv(datastore.cfg.path.input_data.file_paths.cdr)\n",
    "raw_recharges_file = pd.read_csv(datastore.cfg.path.input_data.file_paths.recharges)\n",
    "raw_mobiledata_file = pd.read_csv(datastore.cfg.path.input_data.file_paths.mobiledata)\n",
    "raw_mobilemoney_file = pd.read_csv(datastore.cfg.path.input_data.file_paths.mobilemoney)\n",
    "raw_labels_file = pd.read_csv(datastore.cfg.path.input_data.file_paths.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ac06a-9ce5-4cc0-8996-c7802919dc7f",
   "metadata": {},
   "source": [
    "## Anonymize features\n",
    "\n",
    "We can also anonymize featurized data. This is not necessary (nor will it work) if the features are computed using already-anonymized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbcc69a5-e569-4f89-a0dc-56b4757cdd46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    x1Z7rQbM5O8DbpGg\n",
       "1    x1Z7rQbM5O8DbpGg\n",
       "2    7bLXgD9l64mzAanW\n",
       "3    7bLXgD9l64mzAanW\n",
       "4    MLxA7zdWjO1ybVY1\n",
       "Name: name_anonymized, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anonymizer.anonymize_features()\n",
    "anonymized_features = pd.read_csv(outputs_path / 'outputs' / 'features.csv')\n",
    "anonymized_features.name_anonymized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9bee73-299a-46a4-b06b-4d916ad57cae",
   "metadata": {},
   "source": [
    "## Use a custom format checker\n",
    "\n",
    "You may want to check the format of numbers as you anonymize them. Because the anonymizer uses a hash function, similar numbers will *not* result in similar anonymized strings. For example, the numbers 1234567, 01234567, and 880 1234567 will all result in completely different anonymized strings.\n",
    "\n",
    "The Anonymizer object accepts a format checker when it's constructed. The checker will be evaluated on inputs as strings, and should return `True` if the format is acceptable and `False` if not.\n",
    "\n",
    "Anonymization will fail if any number fails this check. So we encourage you to clean your data prior to passing it through this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78f5c214-d741-442a-bfc0-51bd97b62432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "Loading CDR...\n",
      "Loading recharges...\n",
      "SUCCESS!\n",
      "Loading mobile data...\n",
      "Loading mobile money...\n"
     ]
    }
   ],
   "source": [
    "# A silly example: Some numbers will start with 1, so we'll see validation errors here.\n",
    "def format_checker(raw):\n",
    "    if raw.startswith('1'):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "anonymizer_with_check = Anonymizer(datastore=datastore, format_checker=format_checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f348bce7-56cb-4f59-bc90-3dac81224e27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 10:54:23 ERROR Executor: Exception in task 0.0 in stage 106.0 (TID 106)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/leo/Documents/gpl/cider/cider/anonymizer.py\", line 132, in <lambda>\n",
      "    new_column = udf(\n",
      "  File \"/Users/leo/Documents/gpl/cider/cider/anonymizer.py\", line 180, in _check_identifier_format_and_hash\n",
      "    raise ValueError(f'Bad input to anonymization: {raw_string} rejected by provided format format_checker.')\n",
      "ValueError: Bad input to anonymization: 1275856 rejected by provided format format_checker.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/12 10:54:23 ERROR Executor: Exception in task 1.0 in stage 106.0 (TID 107)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/leo/Documents/gpl/cider/cider/anonymizer.py\", line 132, in <lambda>\n",
      "    new_column = udf(\n",
      "  File \"/Users/leo/Documents/gpl/cider/cider/anonymizer.py\", line 180, in _check_identifier_format_and_hash\n",
      "    raise ValueError(f'Bad input to anonymization: {raw_string} rejected by provided format format_checker.')\n",
      "ValueError: Bad input to anonymization: 1728786 rejected by provided format format_checker.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/12 10:54:23 ERROR TaskSetManager: Task 0 in stage 106.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/leo/Documents/gpl/cider/cider/anonymizer.py\", line 132, in <lambda>\n    new_column = udf(\n  File \"/Users/leo/Documents/gpl/cider/cider/anonymizer.py\", line 180, in _check_identifier_format_and_hash\n    raise ValueError(f'Bad input to anonymization: {raw_string} rejected by provided format format_checker.')\nValueError: Bad input to anonymization: 1275856 rejected by provided format format_checker.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manonymizer_with_check\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manonymize_cdr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gpl/cider/cider/anonymizer.py:88\u001b[0m, in \u001b[0;36mAnonymizer.anonymize_cdr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manonymize_cdr\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_anonymize_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcdr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcaller_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecipient_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gpl/cider/cider/anonymizer.py:138\u001b[0m, in \u001b[0;36mAnonymizer._anonymize_dataset\u001b[0;34m(self, dataset_name, column_names)\u001b[0m\n\u001b[1;32m    132\u001b[0m         new_column \u001b[38;5;241m=\u001b[39m udf(\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m raw: Anonymizer\u001b[38;5;241m.\u001b[39m_check_identifier_format_and_hash(raw, encoder, format_checker), StringType()\n\u001b[1;32m    134\u001b[0m         )(dataset[column_name])\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_anonymized\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;66;03m# using the select function (instead of withColumn) places the new column at the beginning of the df\u001b[39;00m\n\u001b[1;32m    137\u001b[0m         dataset_with_anonymized_columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 138\u001b[0m             dataset_with_anonymized_columns\u001b[38;5;241m.\u001b[39mselect(new_column, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(column_name)\n\u001b[1;32m    139\u001b[0m         )\n\u001b[1;32m    141\u001b[0m save_df(dataset_with_anonymized_columns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/gpl/cider/helpers/utils.py:112\u001b[0m, in \u001b[0;36msave_df\u001b[0;34m(df, out_file_path, sep)\u001b[0m\n\u001b[1;32m    107\u001b[0m temp_folder \u001b[38;5;241m=\u001b[39m out_file_path\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Ask spark to write output there. The repartition(1) call will tell spark to write a single file.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# It will name it with some meaningless partition name, but we can find it easily bc it's the only\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# csv in the temp directory.\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtemp_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m spark_generated_file_name \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    114\u001b[0m     fname \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(temp_folder) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(fname)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    115\u001b[0m ][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# move the file out of the temporary directory and rename it\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cider/lib/python3.8/site-packages/pyspark/sql/readwriter.py:1240\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1223\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1224\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1239\u001b[0m )\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cider/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/envs/cider/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/leo/Documents/gpl/cider/cider/anonymizer.py\", line 132, in <lambda>\n    new_column = udf(\n  File \"/Users/leo/Documents/gpl/cider/cider/anonymizer.py\", line 180, in _check_identifier_format_and_hash\n    raise ValueError(f'Bad input to anonymization: {raw_string} rejected by provided format format_checker.')\nValueError: Bad input to anonymization: 1275856 rejected by provided format format_checker.\n"
     ]
    }
   ],
   "source": [
    "anonymizer_with_check.anonymize_cdr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa67df-d070-44e0-9de1-b029453819e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_cider",
   "language": "python",
   "name": "conda_cider"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
